  0%|                                                                                                                                     | 0/2697 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/rg3637/hpml-assign2/hpml-project/inference_TemplateConstraint.py", line 140, in <module>
    main()
  File "/home/rg3637/hpml-assign2/hpml-project/inference_TemplateConstraint.py", line 93, in main
    outputs = model.generate(
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/generation/utils.py", line 2089, in generate
    generation_config, model_kwargs = self._prepare_generation_config(
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/generation/utils.py", line 1635, in _prepare_generation_config
    model_kwargs = generation_config.update(**kwargs)
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/generation/configuration_utils.py", line 1324, in update
    self.validate()
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/generation/configuration_utils.py", line 715, in validate
    raise ValueError(
ValueError: one of `constraints`, `force_words_ids` is not `None`, triggering constrained beam search. However, `do_sample` is set to `True`, which is incompatible with this generation mode. Set `constraints` and `force_words_ids` to `None` or unset `do_sample` to continue.
