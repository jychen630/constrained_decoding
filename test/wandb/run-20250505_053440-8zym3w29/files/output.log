  0%|                                                                                                                              | 0/2697 [00:00<?, ?it/s]/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
  0%|                                                                                                                    | 1/2697 [00:02<2:06:29,  2.81s/it]/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
  0%|                                                                                                                    | 2/2697 [00:04<1:27:22,  1.95s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
  0%|▏                                                                                                                   | 3/2697 [00:05<1:17:50,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
  0%|▏                                                                                                                   | 4/2697 [00:07<1:11:19,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
  0%|▏                                                                                                                   | 4/2697 [00:07<1:29:41,  2.00s/it]
Traceback (most recent call last):
  File "/home/rg3637/hpml-assign2/hpml-project/inference_TemplateConstraint.py", line 137, in <module>
    main()
  File "/home/rg3637/hpml-assign2/hpml-project/inference_TemplateConstraint.py", line 90, in main
    outputs = model.generate(
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/generation/utils.py", line 2451, in generate
    result = self._constrained_beam_search(
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/generation/utils.py", line 4320, in _constrained_beam_search
    outputs = self(**model_inputs, return_dict=True)
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/accelerate/hooks.py", line 176, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 1062, in forward
    transformer_outputs = self.transformer(
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 922, in forward
    outputs = block(
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/accelerate/hooks.py", line 176, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 404, in forward
    attn_outputs = self.attn(
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/rg3637/anaconda3/envs/hpml/lib/python3.9/site-packages/accelerate/hooks.py", line 176, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/models/gpt2/modeling_gpt2.py", line 335, in forward
    attn_output, attn_weights = attention_interface(
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/integrations/sdpa_attention.py", line 39, in sdpa_attention_forward
    query = query.contiguous()
KeyboardInterrupt
