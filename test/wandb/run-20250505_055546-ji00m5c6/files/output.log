  0%|                                                                                                                                 | 0/5 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/rg3637/hpml-assign2/hpml-project/inference_TemplateConstraint.py", line 133, in <module>
    main()
  File "/home/rg3637/hpml-assign2/hpml-project/inference_TemplateConstraint.py", line 78, in main
    input_encodings = tokenizer(starting_text, padding=True, return_tensors="pt")
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/tokenization_utils_base.py", line 2887, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/tokenization_utils_base.py", line 2997, in _call_one
    return self.encode_plus(
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/tokenization_utils_base.py", line 3064, in encode_plus
    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
  File "/home/rg3637/hpml-assign2/hpml-project/transformers/src/transformers/tokenization_utils_base.py", line 2789, in _get_padding_truncation_strategies
    raise ValueError(
ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
